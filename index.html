<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"luke-blog.netlify.app","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="我爱学习！~~~">
<meta property="og:type" content="website">
<meta property="og:title" content="Luke-blog">
<meta property="og:url" content="https://luke-blog.netlify.app/index.html">
<meta property="og:site_name" content="Luke-blog">
<meta property="og:description" content="我爱学习！~~~">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Luke">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://luke-blog.netlify.app/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Luke-blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Luke-blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Legends never die！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luke-blog.netlify.app/2023/04/13/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/headIcon.jpg">
      <meta itemprop="name" content="Luke">
      <meta itemprop="description" content="我爱学习！~~~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luke-blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/13/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">零基础入门深度学习-循环神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-13 16:59:46" itemprop="dateCreated datePublished" datetime="2023-04-13T16:59:46+08:00">2023-04-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-15 21:06:19" itemprop="dateModified" datetime="2023-04-15T21:06:19+08:00">2023-04-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">零基础入门深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="0x-0-一种形象化的理解方式"><a href="#0x-0-一种形象化的理解方式" class="headerlink" title="0x 0 一种形象化的理解方式"></a>0x 0 一种形象化的理解方式</h2><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415091324304.png" alt="image-20230415091324304"></p>
<p>一定要看这个：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1z5411f7Bm/?spm_id_from=333.337.search-card.all.click&vd_source=be08cd9cc4a3d6f3fec83590352fca21">【循环神经网络】5分钟搞懂RNN，3D动画深入浅出_哔哩哔哩_bilibili</a></p>
<p>前置知识：矩阵对矩阵的求导、矩阵对向量的求导运算</p>
<h2 id="0x-1-RNN能干啥？"><a href="#0x-1-RNN能干啥？" class="headerlink" title="0x 1 RNN能干啥？"></a>0x 1 RNN能干啥？</h2><ul>
<li><p>RNN可以为<strong>语言模型</strong>来建模</p>
</li>
<li><p>比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个<strong>序列</strong>；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：<strong>循环神经网络</strong>(Recurrent Neural Network)。</p>
</li>
</ul>
<h2 id="0x-2-RNN理论知识学习"><a href="#0x-2-RNN理论知识学习" class="headerlink" title="0x 2 RNN理论知识学习"></a>0x 2 RNN理论知识学习</h2><h3 id="最简单的RNN张什么样"><a href="#最简单的RNN张什么样" class="headerlink" title="最简单的RNN张什么样"></a>最简单的RNN张什么样</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415083548108.png" alt="image-20230415083548108"></p>
<p>它会有这样一个式子：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415083623060.png" alt="image-20230415083623060"></p>
<p>反复将式2代入式1，就有：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415083719695.png" alt="image-20230415083719695"></p>
<p>So，输入的前半部分会影响到后半部分的输出。</p>
<h3 id="上点强度——双向RNN"><a href="#上点强度——双向RNN" class="headerlink" title="上点强度——双向RNN"></a>上点强度——双向RNN</h3><p>有两个W，但是这里用A表示。</p>
<p>A1用于正向计算</p>
<p>A2用于反向计算</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415085957116.png" alt="image-20230415085957116"></p>
<h3 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h3><blockquote>
<p>  BPTT算法</p>
</blockquote>
<p>针对循环层的训练算法，原理与BP一致</p>
<p>步骤：</p>
<ol>
<li>前向计算求结果</li>
<li>反向计算求误差</li>
<li>随机下降更权重</li>
</ol>
<h4 id="误差怎么求"><a href="#误差怎么求" class="headerlink" title="误差怎么求"></a>误差怎么求</h4><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E6%A1%A3(10)_6_1681526047319.png" alt="未命名文档(10)_6_1681526047319"></p>
<h4 id="怎么计算权重的梯度？"><a href="#怎么计算权重的梯度？" class="headerlink" title="怎么计算权重的梯度？"></a>怎么计算权重的梯度？</h4><p>博客当中对矩阵求导，但是我们之前几个博客都是对向量进行求导。</p>
<ul>
<li>对向量求导比一个一个对变量求导快（支持GPU加速且形式更美观）</li>
<li>本质相同，但是形式不同</li>
</ul>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415140734980.png" alt="image-20230415140734980"></p>
<p>还是两方面考虑</p>
<ul>
<li>T<sub>k</sub>时刻的前层权重更新就和之前的全连接网络更新梯度一样</li>
<li>T<sub>k</sub>时刻计算T<sub>k-s</sub>的梯度是我们要新考虑的东西</li>
</ul>
<p>权重矩阵U和W的计算方法：</p>
<ul>
<li>∇ 是梯度算子，∇ f ( x )就是指f ( x ) 的梯度。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415111609154.png" alt="image-20230415111609154"></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415111655359.png" alt="image-20230415111655359"></p>
<p>最后为什么是加法呢？见下面的推导</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E6%A1%A3(10)_7_1681561677242.png" alt="未命名文档(10)_7_1681561677242"></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415111906498.png" alt="image-20230415111906498"></p>
<h3 id="RNN的梯度爆炸和消失问题"><a href="#RNN的梯度爆炸和消失问题" class="headerlink" title="RNN的梯度爆炸和消失问题"></a>RNN的梯度爆炸和消失问题</h3><p>梯度消失 &#x3D;&gt; 训练缓慢 甚至停滞</p>
<p>梯度爆炸 &#x3D;&gt; 可能不收敛</p>
<h2 id="RNN的应用——语言模型"><a href="#RNN的应用——语言模型" class="headerlink" title="RNN的应用——语言模型"></a>RNN的应用——语言模型</h2><p>我当前输入一个词，循环神经网络会帮我预测下一个词要输出什么</p>
<p>Solution: 概率。</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415154753640.png" alt="image-20230415154753640"></p>
<p>But,这个概率是怎么来的？</p>
<p>Softmax函数</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415154853403.png" alt="image-20230415154853403"></p>
<p>误差计算</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230415155136887.png" alt="image-20230415155136887"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luke-blog.netlify.app/2023/04/11/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/headIcon.jpg">
      <meta itemprop="name" content="Luke">
      <meta itemprop="description" content="我爱学习！~~~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luke-blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/11/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">零基础入门深度学习 - 卷积神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-11 10:47:20" itemprop="dateCreated datePublished" datetime="2023-04-11T10:47:20+08:00">2023-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-13 16:58:56" itemprop="dateModified" datetime="2023-04-13T16:58:56+08:00">2023-04-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">零基础入门深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="0x-1-它可以做什么？"><a href="#0x-1-它可以做什么？" class="headerlink" title="0x 1 它可以做什么？"></a>0x 1 它可以做什么？</h2><p>图像分类</p>
<h2 id="0x-2-博客学习"><a href="#0x-2-博客学习" class="headerlink" title="0x 2 博客学习"></a>0x 2 博客学习</h2><h3 id="Relu函数-（新的激活函数）"><a href="#Relu函数-（新的激活函数）" class="headerlink" title="Relu函数 （新的激活函数）"></a>Relu函数 （新的激活函数）</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411141548885.png" alt="image-20230411141548885"></p>
<p>它的图像张这样：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411141603148.png" alt="image-20230411141603148"></p>
<blockquote>
<p>  为啥要用Relu?</p>
</blockquote>
<ul>
<li><p>速度快</p>
</li>
<li><p><mark>减轻梯度消失问题(啥意思？)</mark></p>
</li>
<li><p>稀疏性</p>
</li>
</ul>
<h3 id="啥是卷积神经网络"><a href="#啥是卷积神经网络" class="headerlink" title="啥是卷积神经网络"></a>啥是卷积神经网络</h3><blockquote>
<p>  感性认识</p>
</blockquote>
<p>一张彩色图片 &#x3D; 3-D tensor</p>
<p>哪三个d呢？</p>
<ul>
<li>图片宽 W</li>
<li>图片高 H</li>
<li>图片的 channels</li>
</ul>
<p>图像分类其实不需要使用全连接，原因是：我只需要图像当中个别的几个特征（<mark>非常有辨识度的</mark>），就能够辨别出这是个什么玩意，就是下图所示的这种感觉~~</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411110443341.png" alt="image-20230411110443341"></p>
<p>使用全连接就有些像扔给神经元的全是完整的图片。</p>
<p>怎么做呢？</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411111559626.png" alt="image-20230411111559626"></p>
<p>同样的特征可能出现图片当中的不同地方，像上面的这种全地图扫描的方式，确实不会漏掉特征，但是还是整个系统还是有些庞大~</p>
<p>Solution: 共享参数（weight一样），但是field不一样</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411134729710.png" alt="image-20230411134729710"></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411143558503.png" alt="image-20230411143558503"></p>
<p>Q：从input image 到 feature maps    1-&gt;3的这个数量变化是怎么来的？</p>
<p>A: </p>
<ul>
<li>每一个filter进行一次卷积后都会得到一个Feature map,所以从input image 到 feature maps应该是有三个filter。</li>
</ul>
<p>简单说：</p>
<ul>
<li>Convolution Layer使用Filter得到Feature Maps    (寻找特征的过程)</li>
<li>Pooling Layer是对Feature Map进行采样的过程 (寻找最明显特征的过程)</li>
</ul>
<h3 id="Filter是怎么工作的？"><a href="#Filter是怎么工作的？" class="headerlink" title="Filter是怎么工作的？"></a>Filter是怎么工作的？</h3><p>stride &#x3D; 1（步幅为1）</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/20190715200135421.gif" alt="20190715200135421"></p>
<p>stride &#x3D; 2时：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411150607336.png" alt="image-20230411150607336"></p>
<p>所以说，最终的Feature Map大小有个公式</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411150645429.png" alt="image-20230411150645429"></p>
<ul>
<li><em>W</em>2是卷积后Feature Map的宽度</li>
<li><em>W</em>1是卷积前图像的宽度</li>
<li><em>F</em>是filter的宽度</li>
<li><em>P</em>是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果的值是1，那么就补1圈0；</li>
<li><em>S</em>是步幅；</li>
<li><em>H</em>2是卷积后Feature Map的高度；</li>
<li>H1是卷积前图像的宽度</li>
</ul>
<h3 id="深度为1的卷积层计算方法："><a href="#深度为1的卷积层计算方法：" class="headerlink" title="深度为1的卷积层计算方法："></a>深度为1的卷积层计算方法：</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411152036255.png" alt="image-20230411152036255"></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/20190715200135421.gif" alt="20190715200135421"></p>
<h3 id="深度大于1的卷积层计算方法："><a href="#深度大于1的卷积层计算方法：" class="headerlink" title="深度大于1的卷积层计算方法："></a>深度大于1的卷积层计算方法：</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230412145431694.png" alt="image-20230412145431694"></p>
<p>例如：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/20190715200149945.gif" alt="20190715200149945"></p>
<p>上面的图意思就是：有三个Channel，两个filter的计算方法，立体一点的话，就是这样子：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230412145713517.png" alt="image-20230412145713517"></p>
<p>but，上面的公式是不是有点复杂，能不能精简一下？</p>
<p>能，但是只能在stride&#x3D;1的情况下进行精简——卷积公式</p>
<blockquote>
<p>  卷积公式</p>
</blockquote>
<p>我们在概率论当中学习过卷积公式，但是数学的卷积公式和卷积神经网络当中的卷积是有区别的：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E6%97%A0%E6%A0%87%E9%A2%98.png" alt="无标题"></p>
<h3 id="Pooling-层输出值的计算"><a href="#Pooling-层输出值的计算" class="headerlink" title="Pooling 层输出值的计算"></a>Pooling 层输出值的计算</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411154043592.png" alt="image-20230411154043592"></p>
<p>取出Feature Map当中最重要的样本，从而减少参数数量。</p>
<p>这里用的是Max Pooling。</p>
<p>其他的还有Mean Pooling….</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>没啥说的</p>
<h3 id="卷积神经网络的训练"><a href="#卷积神经网络的训练" class="headerlink" title="卷积神经网络的训练"></a>卷积神经网络的训练</h3><p>和全连接神经网络相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的：</p>
<p>利用链式求导计算损失函数<mark>对每个权重的偏导数（梯度）</mark>，然后根据梯度下降公式更新权重。训练算法依然是反向传播算法。</p>
<blockquote>
<p>  卷积层的<mark>误差项</mark>传递   (注意是误差项(E<sub>d</sub>对net求导)，不是误差(E<sub>d</sub>))</p>
</blockquote>
<p>步长为1时的误差</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E6%A1%A3(10)_1_1681296821894.png" alt="未命名文档(10)_1_1681296821894"></p>
<p>步长&gt;1时的误差(从步长为1的里面挑一挑就是了)：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230412185529024.png" alt="image-20230412185529024"></p>
<blockquote>
<p>  卷积层 filter权重梯度的计算（有疑问）</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E6%A1%A3(10)_2_1681299279585.png" alt="未命名文档(10)_2_1681299279585"></p>
<p>理解：image经过多个filter会产生多个channel，如果做反向传播的话，生成的三个channel对原来的image的误差是有”叠加“效果的。</p>
<blockquote>
<p>  Pooling层的训练</p>
</blockquote>
<p>无论max pooling还是mean pooling，都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算。</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E6%A1%A3(10)_5_1681301675670.png" alt="未命名文档(10)_5_1681301675670"></p>
<h2 id="0x-3-python小知识"><a href="#0x-3-python小知识" class="headerlink" title="0x 3 python小知识"></a>0x 3 python小知识</h2><h3 id="random函数的用法"><a href="#random函数的用法" class="headerlink" title="random函数的用法"></a>random函数的用法</h3><p>code：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">weights = np.random.uniform(-<span class="number">1e-4</span>,<span class="number">1e-4</span>,(<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">bias = <span class="number">0</span></span><br><span class="line">weight_grad = np.zeros(weights.shape)</span><br><span class="line"><span class="built_in">print</span>(weight_grad)</span><br></pre></td></tr></table></figure>

<p>output:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[[0. 0. 0.]</span><br><span class="line">  [0. 0. 0.]</span><br><span class="line">  [0. 0. 0.]]</span><br><span class="line"></span><br><span class="line"> [[0. 0. 0.]</span><br><span class="line">  [0. 0. 0.]</span><br><span class="line">  [0. 0. 0.]]]</span><br></pre></td></tr></table></figure>





<h3 id="魔术方法-repr"><a href="#魔术方法-repr" class="headerlink" title="魔术方法 _repr_"></a>魔术方法 _<em>repr</em>_</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230413081441432.png" alt="image-20230413081441432"></p>
<p>就是说，实例化一个类的时候，会有一个回显</p>
<h3 id="np-nditer-函数"><a href="#np-nditer-函数" class="headerlink" title="np.nditer()函数"></a>np.nditer()函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># np.arange(6) 得到：[0 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 再reshape一下，得到：[[0 1 2] [3 4 5]]</span></span><br><span class="line"><span class="keyword">with</span> np.nditer(a,op_flags = [<span class="string">&#x27;readwrite&#x27;</span>]) <span class="keyword">as</span> it: <span class="comment">#np.nditer(arrar,op_flags)会生成一个迭代器，我们叫他it</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> it:    <span class="comment">#使用nditer迭代器进行循环，速度要比for-loop快很多倍</span></span><br><span class="line">        x[...] = <span class="number">2</span>*x <span class="comment">#x[...]是一种“扩展切片”（Ellipsis slicing）的写法，它表示对数组中所有元素进行操作。</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="在做padding的时候的代码解析："><a href="#在做padding的时候的代码解析：" class="headerlink" title="在做padding的时候的代码解析："></a>在做padding的时候的代码解析：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> input_array.ndim == <span class="number">3</span>:   <span class="comment">#ndim 是数组的维度</span></span><br><span class="line">    input_width = input_array.shape[<span class="number">2</span>]</span><br><span class="line">    input_height = input_array.shape[<span class="number">1</span>]</span><br><span class="line">    input_depth = input_array.shape[<span class="number">0</span>]</span><br><span class="line">    padded_array = np.zeros((</span><br><span class="line">        input_depth,</span><br><span class="line">        input_height + <span class="number">2</span>*zp,</span><br><span class="line">        input_width + <span class="number">2</span>*zp</span><br><span class="line">    ))</span><br><span class="line">    padded_array[:,</span><br><span class="line">        zp:zp + input_height,</span><br><span class="line">        zp:zp +input_width</span><br><span class="line">    ] = input_array</span><br><span class="line">    <span class="keyword">return</span> padded_array</span><br><span class="line"><span class="keyword">elif</span> input_array.ndim==<span class="number">2</span>:</span><br><span class="line">    input_width = input_array.shape[<span class="number">1</span>]</span><br><span class="line">    input_height = input_array.shpe[<span class="number">0</span>]</span><br><span class="line">    padded_array = np.zeros((</span><br><span class="line">        input_height + <span class="number">2</span>*zp,</span><br><span class="line">        input_width + <span class="number">2</span>*zp</span><br><span class="line">    ))</span><br><span class="line">    padded_array[</span><br><span class="line">        zp:zp+input_height+<span class="number">2</span>*zp,</span><br><span class="line">        zp:zp + input_width</span><br><span class="line">    ] = input_array</span><br><span class="line">    <span class="keyword">return</span> padded_array</span><br></pre></td></tr></table></figure>

<p>核心考察点：numpy数组当中，冒号的使用</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230413093313261.png" alt="image-20230413093313261"></p>
<p>简单说流程： 找到填充的位置，进行填充</p>
<h3 id="使用numpy数组的时候，它完全就是一个向量-x2F-矩阵"><a href="#使用numpy数组的时候，它完全就是一个向量-x2F-矩阵" class="headerlink" title="使用numpy数组的时候，它完全就是一个向量&#x2F;矩阵"></a>使用numpy数组的时候，它完全就是一个向量&#x2F;矩阵</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230413111439816.png" alt="image-20230413111439816"></p>
<p>可以去做向量减法、矩阵乘法等等操作。。。</p>
<h3 id="numpy数组的shape属性"><a href="#numpy数组的shape属性" class="headerlink" title="numpy数组的shape属性"></a>numpy数组的shape属性</h3><p>二维：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230413145942055.png" alt="image-20230413145942055"></p>
<p>shape[0] ：有几行</p>
<p>shape[1]：有几列</p>
<p>三维：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230413150004377.png" alt="image-20230413150004377"></p>
<p>shape[0] ：有几层 （channel数）</p>
<p>shape[1] ： 有几行</p>
<p>shape[2] : 有几列 </p>
<h2 id="0x-4-代码实现盘逻辑-amp-debug"><a href="#0x-4-代码实现盘逻辑-amp-debug" class="headerlink" title="0x 4 代码实现盘逻辑 &amp; debug"></a>0x 4 代码实现盘逻辑 &amp; debug</h2><p>以下为矢量图，可以下载下来放大看</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9E%E7%8E%B0.svg" alt="卷积神经网络的实现"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luke-blog.netlify.app/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/headIcon.jpg">
      <meta itemprop="name" content="Luke">
      <meta itemprop="description" content="我爱学习！~~~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luke-blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">零基础入门深度学习 - 神经网络和反向传播算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-09 20:25:02" itemprop="dateCreated datePublished" datetime="2023-04-09T20:25:02+08:00">2023-04-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-15 08:45:34" itemprop="dateModified" datetime="2023-04-15T08:45:34+08:00">2023-04-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">零基础入门深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1MP4y1E7vu?t=805.2&p=72">最简单的理解方法</a></p>
<h2 id="0x0-前置知识学的有啥用？"><a href="#0x0-前置知识学的有啥用？" class="headerlink" title="0x0 前置知识学的有啥用？"></a>0x0 前置知识学的有啥用？</h2><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410195117236.png" alt="image-20230410195117236"></p>
<h2 id="0x1-这节课学的东西，它能干啥呢❓"><a href="#0x1-这节课学的东西，它能干啥呢❓" class="headerlink" title="0x1 这节课学的东西，它能干啥呢❓"></a>0x1 这节课学的东西，它能干啥呢❓</h2><p>手写数字识别？？！！</p>
<h2 id="0x2-怎么搞？？"><a href="#0x2-怎么搞？？" class="headerlink" title="0x2 怎么搞？？"></a>0x2 怎么搞？？</h2><p>前面学习的都是训练<mark>单个神经元</mark>，回顾一下，前面干了些什么：</p>
<ul>
<li>感知器模拟and运算</li>
<li>预测工资</li>
</ul>
<p>现在学习的，就是将这些单个神经元（感知器）链接在一起，形成神经网络！</p>
<p>糟了，感觉太神奇了！</p>
<h2 id="0x3-理论学习🖊"><a href="#0x3-理论学习🖊" class="headerlink" title="0x3 理论学习🖊"></a>0x3 理论学习🖊</h2><h3 id="神经元-V-S-感知器"><a href="#神经元-V-S-感知器" class="headerlink" title="神经元 V.S. 感知器"></a>神经元 V.S. 感知器</h3><p>鲁迅曾说过：神经元与感知器虽说一样，但有点不一样🙈</p>
<ul>
<li>不一样点：神经元激活函数往往选择为sigmoid函数或tanh函数，而我们说感知器的时候，它的激活函数是阶跃函数</li>
</ul>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409204803400.png" alt="image-20230409204803400"></p>
<h3 id="啥是神经网络？"><a href="#啥是神经网络？" class="headerlink" title="啥是神经网络？"></a>啥是神经网络？</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409204400130.png" alt="image-20230409204400130"></p>
<p>上面这种是<mark>全连接神经网络的结构</mark>。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。</p>
<h3 id="神经网络怎么干活？"><a href="#神经网络怎么干活？" class="headerlink" title="神经网络怎么干活？"></a>神经网络怎么干活？</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409204400130.png" alt="image-20230409204400130"></p>
<ul>
<li><p>如何计算a4的值？</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409205221141.png" alt="image-20230409205221141"></p>
<p>上面的式子咋来的？</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409205401484.png" alt="image-20230409205401484"></p>
<ul>
<li>如何计算y1?</li>
</ul>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409205456810.png" alt="image-20230409205456810"></p>
<p>上面的式子咋来的？</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409205530209.png" alt="image-20230409205530209"></p>
<ul>
<li>来波小小的summary</li>
</ul>
<p>简单讲： 输入是<img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409205650061.png" alt="image-20230409205650061"> 输出是<img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409205710388.png" alt="image-20230409205710388">，怎么做到的——神经网络~</p>
<p>如何用数学来理解？</p>
<p>hidden layer实际上是一个matrix， 向量进去之后，空间发生了扭曲变换，上面的这个变换就像是“二向箔”，形成了新的vector</p>
<h3 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h3><p>神经网络是一个<strong>模型</strong></p>
<p>权值是模型的<strong>参数</strong></p>
<p>一个神经网络的<strong>连接方式、网络的层数、每层的节点数</strong>这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为**超参数(Hyper-Parameters)**。</p>
<h3 id="反向传播算法-通过结果看原因"><a href="#反向传播算法-通过结果看原因" class="headerlink" title="反向传播算法(通过结果看原因)"></a>反向传播算法(通过结果看原因)</h3><p>⭐⭐⭐⭐<strong>一句话说</strong>：从后向前，利用误差项的计算和权重更新方法，计算出w<sub>ji</sub>更新所需要用到的δ</p>
<blockquote>
<p>   啥意思？为啥叫反向传播算法？</p>
</blockquote>
<p>举个有监督模型的例子：<strong>高中语文阅读理解题（特点：主观性强）</strong></p>
<p>简单说：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410080326438.png" alt="image-20230410080326438"></p>
<ol>
<li><p>有监督&#x3D;&gt;有答案的练习册，我做完题之后可以对答案</p>
</li>
<li><p>对答案时，我发现我写的不太好</p>
<p>怎么不好呢？&#x3D;&gt; 输出的Y 和我 答案上的 T相差比较大 (通过误差项估计可以看出来)</p>
</li>
<li><p>那<strong>写的不好</strong>（<strong>output layer</strong>），我就要找找原因了，也许是<strong>学习方法的问题</strong> </p>
<ol>
<li>我先根据误差，对自己写的答案进行了一点修改 （到了<strong>hidden layer</strong>）</li>
<li>修改了一点后，我悟了，原来是最开始的时候，审题审的不好，那么再根据误差修改我的审题思路(到了 <strong>input layer</strong>)</li>
</ol>
</li>
</ol>
<blockquote>
<p>   Q&amp;A:</p>
</blockquote>
<p>Q：我改的是什么？</p>
<p>A：改的其实是每一个神经元的w,也就是权重</p>
<p>Q：改完之后，能干啥啊？</p>
<p>A：改完之后，输出就更接近真实的答案了。给分也就会更高。</p>
<blockquote>
<p>  用数学理解 反向传播算法（🐕🐕🐕）</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410080326438.png" alt="image-20230410080326438"></p>
<p><strong>用矩阵来描述上面的这个神经网络：</strong></p>
<p>令：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410200957859.png" alt="image-20230410200957859"></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410201019107.png" alt="image-20230410201019107"></p>
<p>从而：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410201102748.png" alt="image-20230410201102748"></p>
<p>最终有：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410201124073.png" alt="image-20230410201124073"></p>
<p><strong>更新权值相关公式（这在5.2.1、5.2.2中有推导过程）：</strong></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410195407536.png" alt="image-20230410195407536"></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410195428081.png" alt="image-20230410195428081"></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410195503148.png" alt="image-20230410195503148"></p>
<p>能用这个式子的理由：梯度下降法</p>
<p><mark>这段话非常重要：</mark>显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后，我们就可以根据<strong>式5</strong>来更新所有的权重。</p>
<p>看看，式子5当中的</p>
<ul>
<li><p><em>η</em>是学习速率</p>
</li>
<li><p><em>δ</em> <sub><em>j</em></sub>是节点j 的误差项（<mark>注意:</mark>误差项和误差不是一个东西）</p>
</li>
<li><p>x<sub>ji</sub>是节点i传递给节点j的输入</p>
</li>
<li><p>我的目的：让输出T与Y的误差变小</p>
<ul>
<li><p>误差怎么估计？ &#x3D;&gt; <img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410091217289.png" alt="image-20230410091217289"></p>
</li>
<li><p>梯度下降法：出误差E <sub>d</sub>对于每个权重w<sub>ji</sub>的偏导数（也就是梯度）<img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410193604139.png" alt="image-20230410193604139" style="zoom: 67%;" />这就有回到了式(5)</p>
</li>
<li><p>计算出误差E <sub>d</sub>对于每个权重w<sub>ji</sub>的偏导数的目的：修改wji</p>
</li>
<li><p>修改w<sub>ji</sub>的目的：让误差变小</p>
</li>
</ul>
</li>
</ul>
<h2 id="0x4-python语法知识补充-🎈"><a href="#0x4-python语法知识补充-🎈" class="headerlink" title="0x4 python语法知识补充 🎈"></a>0x4 python语法知识补充 🎈</h2><h3 id="a-1-V-S-a-1-V-S-a-1"><a href="#a-1-V-S-a-1-V-S-a-1" class="headerlink" title="a[-1] V.S. a[:-1] V.S. a[::-1]"></a>a[-1] V.S. a[:-1] V.S. a[::-1]</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230410144632783.png" alt="image-20230410144632783"></p>
<h2 id="0x5-写写代码、搞懂整体逻辑"><a href="#0x5-写写代码、搞懂整体逻辑" class="headerlink" title="0x5 写写代码、搞懂整体逻辑"></a>0x5 写写代码、搞懂整体逻辑</h2><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E7%9B%98%E4%B8%80%E7%9B%98%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E7%9A%84%E9%80%BB%E8%BE%91%20%E6%A0%B8%E5%BF%83%EF%BC%9ANetwork%E7%B1%BB.svg" alt="盘一盘反向传播算法和代码实现的逻辑 核心：Network类"></p>
<h2 id="0x6-如何保证自己的神经网络没有BUG"><a href="#0x6-如何保证自己的神经网络没有BUG" class="headerlink" title="0x6 如何保证自己的神经网络没有BUG?"></a>0x6 如何保证自己的神经网络没有BUG?</h2><p>核心：</p>
<p>将神经网络代码算出来的梯度值和手算出来的梯度值进行比较，如果差别比较小，就说明代码是正确的。</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411090826503.png" alt="image-20230411090826503"></p>
<h2 id="0x7-手写数字识别"><a href="#0x7-手写数字识别" class="headerlink" title="0x7 手写数字识别"></a>0x7 手写数字识别</h2><p><strong>准备数据集：MNIST数据集</strong></p>
<h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>超参数（hyperparameters）是指机器学习模型中需要手动设置的参数，这些参数不能通过训练数据自动学习得到。 通常，超参数需要在训练过程前手动调整，以优化模型的性能和准确性。</p>
<p>超参数通常包括<mark>模型的学习率、正则化参数、批量大小、层数、神经元数量等</mark>。这些参数的设置可以对模型的效果产生重要影响，因此超参数的选择是非常重要的。</p>
<p>为了找到最佳的超参数组合，可以使用交叉验证等技术来评估模型的性能，并尝试不同的超参数值来找到最佳的超参数组合。</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411091321710.png" alt="image-20230411091321710"></p>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>错误率&#x3D;错误预测样本数&#x2F;总样本数</p>
<h2 id="0x8-向量化编程"><a href="#0x8-向量化编程" class="headerlink" title="0x8 向量化编程"></a>0x8 向量化编程</h2><p>区别于面向对象、面向过程</p>
<p>why? &#x3D;&gt; GPU可以对向量运算进行优化</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230411092620519.png" alt="image-20230411092620519"></p>
<p>通过上面的这一个公式，来实现全连接层向前和向后计算</p>
<p>对比面向对象的写法：</p>
<p>面向对象是一步一步向前面的Layer迭代的，向量化是对整个网络直接操作的</p>
<h2 id="0x-9手推公式"><a href="#0x-9手推公式" class="headerlink" title="0x 9手推公式"></a>0x 9手推公式</h2><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E6%A1%A3(10)_2_1681288291648.png" alt="未命名文档(10)_2_1681288291648"></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E6%A1%A3(10)_3_1681288363607.png" alt="未命名文档(10)_3_1681288363607"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luke-blog.netlify.app/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/headIcon.jpg">
      <meta itemprop="name" content="Luke">
      <meta itemprop="description" content="我爱学习！~~~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luke-blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">零基础入门深度学习 - 梯度下降算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-09 15:29:31" itemprop="dateCreated datePublished" datetime="2023-04-09T15:29:31+08:00">2023-04-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-11 16:17:26" itemprop="dateModified" datetime="2023-04-11T16:17:26+08:00">2023-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">零基础入门深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="0x1-这是要干啥？？？🐕🐕🐕"><a href="#0x1-这是要干啥？？？🐕🐕🐕" class="headerlink" title="0x1 这是要干啥？？？🐕🐕🐕"></a>0x1 这是要干啥？？？🐕🐕🐕</h2><h3 id="目的：我想让误差函数的值变小，咋办？"><a href="#目的：我想让误差函数的值变小，咋办？" class="headerlink" title="目的：我想让误差函数的值变小，咋办？"></a>目的：我想让误差函数的值变小，咋办？</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409154853341.png" alt="image-20230409154853341"></p>
<p>E是可导函数。</p>
<p>求导！找极小值！然后再看端点！</p>
<h2 id="0x2-画画重点"><a href="#0x2-画画重点" class="headerlink" title="0x2 画画重点"></a>0x2 画画重点</h2><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><blockquote>
<p>  简单说：</p>
</blockquote>
<p>计算机不会求极值，but可导函数的极值有个特点：导数为0。</p>
<p>所以让计算机遍历函数，find极小值</p>
<blockquote>
<p>  涉及的几个关键词：</p>
</blockquote>
<p><mark>梯度下降算法的公式</mark>：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155120140.png" alt="image-20230409155120140">(式1)</p>
<p>那么，我要想求E(w)的极值点，f(x)换成E(w)就行了</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155235721.png" alt="image-20230409155235721">(式2)</p>
<p>∇ E(w)的推导过程目前先不管，它是这样算的：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155359570.png" alt="image-20230409155359570">(式3)</p>
<p>So,把式子3代入式子2就得到了：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155519593.png" alt="image-20230409155519593">(式4)</p>
<p>Easy啊~ 一点强度没有！</p>
<h3 id="误差推导"><a href="#误差推导" class="headerlink" title="误差推导"></a>误差推导</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011681952/article/details/93714665">(24条消息) 零基础入门深度学习(2) - 线性单元和梯度下降_Godswisdom的博客-CSDN博客</a></p>
<p><mark>疑惑点：对整个向量求导是什么意思？</mark></p>
<h3 id="随机梯度下降算法-SGD算法"><a href="#随机梯度下降算法-SGD算法" class="headerlink" title="随机梯度下降算法(SGD算法)"></a>随机梯度下降算法(SGD算法)</h3><blockquote>
<p>  why?????</p>
</blockquote>
<p>按照上面提到的<img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155359570.png" alt="image-20230409155359570">来寻找best参数，计算量非常大，更好的solution是SGD算法</p>
<blockquote>
<p>  怎么做的(没有详细介绍)</p>
</blockquote>
<p>在SGD算法中，<mark>每次更新的迭代，只计算一个样本</mark>。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更w 新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新w 并不一定按照减少E 的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少E 的方向前进的，因此最后也能收敛到最小值附近。</p>
<p>对比BGD：</p>
<p><mark>理解：SGD工作的维度是1维，但是次数很多。BGD工作维度为1000000维（夸张的手法），但是次数很少</mark></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luke-blog.netlify.app/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/headIcon.jpg">
      <meta itemprop="name" content="Luke">
      <meta itemprop="description" content="我爱学习！~~~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luke-blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83/" class="post-title-link" itemprop="url">零基础入门深度学习- 线性单元</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-09 14:45:12" itemprop="dateCreated datePublished" datetime="2023-04-09T14:45:12+08:00">2023-04-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-11 07:39:45" itemprop="dateModified" datetime="2023-04-11T07:39:45+08:00">2023-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">零基础入门深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="0x1这是要干啥？？？"><a href="#0x1这是要干啥？？？" class="headerlink" title="0x1这是要干啥？？？"></a>0x1这是要干啥？？？</h2><p>通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就<mark>掌握了机器学习的基本套路</mark></p>
<h2 id="0x2-自己画画重点"><a href="#0x2-自己画画重点" class="headerlink" title="0x2 自己画画重点"></a>0x2 自己画画重点</h2><h3 id="线性单元"><a href="#线性单元" class="headerlink" title="线性单元"></a>线性单元</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409145447181.png" alt="image-20230409145447181"></p>
<h3 id="啥是模型？"><a href="#啥是模型？" class="headerlink" title="啥是模型？"></a>啥是模型？</h3><p>当我们说<strong>模型</strong>时，我们实际上在谈论根据输入x 预测输出y 的<mark><strong>算法</strong></mark></p>
<p><mark>e.g:</mark><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409145745109.png" alt="image-20230409145745109"></p>
<p>如果将 b 写成 w1*x1 ，<mark>其中 w1 &#x3D; b 且 x1 &#x3D; 1</mark></p>
<p>那么y &#x3D; w*x + w1* x1</p>
<p>这个式子还可以写成向量的形式：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409150039172.png" alt="image-20230409150039172"></p>
<p> <mark>长成这种样子模型就叫做线性模型</mark></p>
<h3 id="监督学习-amp-无监督学习"><a href="#监督学习-amp-无监督学习" class="headerlink" title="监督学习&amp;无监督学习"></a>监督学习&amp;无监督学习</h3><p>简单讲：</p>
<ul>
<li>监督学习 -&gt; 有label<ul>
<li>类比 一本有答案的习题册，刷完这套题之后去考试（效果嘎嘎好！）</li>
</ul>
</li>
<li>无监督学习 -&gt; 没有 label <ul>
<li>类比一本没有答案的习题册，刷完之后去考试（效果不太好）</li>
</ul>
</li>
</ul>
<h3 id="如何理解输入？"><a href="#如何理解输入？" class="headerlink" title="如何理解输入？"></a>如何理解输入？</h3><p>类比  我向习题册写的东西，函数的x</p>
<p>如何理解<mark>label</mark>? </p>
<p>类比  习题册的答案、图像识别的工程帽标签、函数的y</p>
<h3 id="线性单元的目标函数（讲这个的时候，只考虑了监督学习）"><a href="#线性单元的目标函数（讲这个的时候，只考虑了监督学习）" class="headerlink" title="线性单元的目标函数（讲这个的时候，只考虑了监督学习）"></a>线性单元的目标函数（讲这个的时候，只考虑了监督学习）</h3><p><mark>理解方式：拟合</mark></p>
<p>有了特征x,通过模型h(x)可以计算出来y的估计值。</p>
<h3 id="误差的计算"><a href="#误差的计算" class="headerlink" title="误差的计算"></a>误差的计算</h3><p><mark>理解： 真值和预测值之间差距有多大啊？ 越大就说明这个模型越不好！</mark></p>
<p>以下推导是非常自然的，一点点看即可~~</p>
<ul>
<li>单个样本的误差<ul>
<li><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409151300387.png" alt="image-20230409151300387"></li>
</ul>
</li>
<li>N个样本误差的合（注意，以下的几个式子都一样）<ul>
<li><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409151559800.png" alt="image-20230409151559800"><ul>
<li>上面的<mark>y拔<mark>就是模型的预测值：<img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409151616024.png" alt="image-20230409151616024"></li>
</ul>
</li>
<li>最终版本：🐕🐕🐕<img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409151803612.png" alt="image-20230409151803612"></li>
</ul>
</li>
</ul>
<h2 id="0x3盘一下逻辑⭐⭐⭐⭐⭐"><a href="#0x3盘一下逻辑⭐⭐⭐⭐⭐" class="headerlink" title="0x3盘一下逻辑⭐⭐⭐⭐⭐"></a>0x3盘一下逻辑⭐⭐⭐⭐⭐</h2><p>开局先自己搞个函数</p>
<p>e.g.: </p>
<p>y &#x3D; ax1+bx2+cx3</p>
<p>y &#x3D; W<sup>T</sup>X</p>
<p>我说这个函数就能解决能<mark>预测房价</mark>，ok吗？</p>
<p>答：ok的，我把6月的房价(x1)、销售量(x2)、地段(x3)输入到上面的函数当中，就能完成预测。</p>
<p>but这<mark>估计是不准确</mark>的，但是能让他准点吗？</p>
<p>答：能！ <mark>有监督学习 + 误差计算</mark>不断修改W<sup>T</sup>的值，就能让他变准点！</p>
<h2 id="0x4-简单写一下代码，里面有些东西要记住"><a href="#0x4-简单写一下代码，里面有些东西要记住" class="headerlink" title="0x4 简单写一下代码，里面有些东西要记住~"></a>0x4 简单写一下代码，里面有些东西要记住~</h2><p>Python当中的继承</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f = <span class="keyword">lambda</span> x: x</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearUnit</span>(<span class="title class_ inherited__">Perceptron</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_num</span>):</span><br><span class="line">        Perceptron.__init__(self, input_num, f)</span><br></pre></td></tr></table></figure>








      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luke-blog.netlify.app/2023/04/08/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/headIcon.jpg">
      <meta itemprop="name" content="Luke">
      <meta itemprop="description" content="我爱学习！~~~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luke-blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/08/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E5%99%A8/" class="post-title-link" itemprop="url">零基础入门深度学习-感知器</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-08 21:17:39" itemprop="dateCreated datePublished" datetime="2023-04-08T21:17:39+08:00">2023-04-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-11 07:39:34" itemprop="dateModified" datetime="2023-04-11T07:39:34+08:00">2023-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">零基础入门深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="0x1-理解"><a href="#0x1-理解" class="headerlink" title="0x1 理解"></a>0x1 理解</h2><p>链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011681952/article/details/93633608">https://blog.csdn.net/u011681952/article/details/93633608</a><br>我的理解： 核心是函数<br>但是要对这个函数进行<mark>调参</mark><br>从而能实现线性分类或者线性回归问题<br>e.g. and 函数、or 函数</p>
<h2 id="0x2-从这节课当中学习到的知识"><a href="#0x2-从这节课当中学习到的知识" class="headerlink" title="0x2 从这节课当中学习到的知识"></a>0x2 从这节课当中学习到的知识</h2><blockquote>
<p>  填充list</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weight = [0.0 for _ in range(10)]</span><br><span class="line">print(weight)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>  python当中的类（类比Java学习）</p>
</blockquote>
<p><mark>核心：构造方法、this关键字</mark></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230408200856228.png" alt="image-20230408200856228"></p>
<p><mark>类比：toString方法</mark></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230408201327525.png" alt="image-20230408201327525"></p>
<p>这里有个知识点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\t 是指制表符，代表着四个空格，也就是一个tab</span><br><span class="line">%s %f 是字符串的格式方式</span><br><span class="line">self.weights会被填充到 %s的位置上</span><br><span class="line">self.bias 会被填充到 %f的位置上</span><br></pre></td></tr></table></figure>



<p><mark>普通方法：</mark></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230408203210295.png" alt="image-20230408203210295"></p>
<p>这里有个知识点：</p>
<ul>
<li><p>activator是在构造函数中有定义，由于python没有数值的类型要求，所以不需要在函数内部再去单独定义这个类的变量</p>
</li>
<li><p>lambda表达式的使用</p>
<ul>
<li>lambda 参数：返回值	 <mark>e.g. lambda a,b:a+b</mark></li>
<li>写上它，就相当于说给了一个函数<ul>
<li>这个函数可以用在reduce函数当中，进行迭代（目前知道的用途）</li>
</ul>
</li>
</ul>
</li>
<li><p>map的使用</p>
<ul>
<li><p>map(function, iterable, …)</p>
</li>
<li><p>它返回的是一个Map对象，如果想要输出里面的内容，可以转成List集合进行输出</p>
</li>
<li><p>function —-&gt; 函数<br>iterable —-&gt; 一个或多个序列</p>
</li>
<li><p><mark>经典例子</mark></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=一般写法：&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=</span></span><br><span class="line"><span class="comment"># 1、计算平方数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">square</span>(<span class="params">x</span>):</span><br><span class="line">	<span class="keyword">return</span> x ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">map</span>(square, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])	<span class="comment"># 计算列表各个元素的平方</span></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=匿名函数写法：&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;</span></span><br><span class="line"><span class="comment"># 2、计算平方数，lambda 写法</span></span><br><span class="line"><span class="built_in">map</span>(<span class="keyword">lambda</span> x: x ** <span class="number">2</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>]	 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、提供两个列表，将其相同索引位置的列表元素进行相加</span></span><br><span class="line"><span class="built_in">map</span>(<span class="keyword">lambda</span> x, y: x + y, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>])</span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">[<span class="number">3</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">19</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>zip函数的使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">z = [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"> </span><br><span class="line">xyz = <span class="built_in">zip</span>(x, y, z)</span><br><span class="line"><span class="comment">#print xyz运行的结果是：</span></span><br><span class="line">[(<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>), (<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>), (<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>)]</span><br></pre></td></tr></table></figure>
</li>
<li><p>reduce函数的使用</p>
<ul>
<li><p>描述：<strong>reduce()</strong> 函数会对参数序列中元素进行累积。</p>
<p>函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。</p>
</li>
<li><p>&#96;&#96;&#96;python</p>
<h1 id="用法："><a href="#用法：" class="headerlink" title="用法："></a>用法：</h1><p>reduce(function, iterable[, initializer])</p>
<h1 id="function-–-函数，有两个参数"><a href="#function-–-函数，有两个参数" class="headerlink" title="function – 函数，有两个参数"></a>function – 函数，有两个参数</h1><h1 id="iterable-–-可迭代对象"><a href="#iterable-–-可迭代对象" class="headerlink" title="iterable – 可迭代对象"></a>iterable – 可迭代对象</h1><h1 id="initializer-–-可选，初始参数"><a href="#initializer-–-可选，初始参数" class="headerlink" title="initializer – 可选，初始参数"></a>initializer – 可选，初始参数</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+   例子：</span><br><span class="line"></span><br><span class="line">    ```python</span><br><span class="line">    #!/usr/bin/python</span><br><span class="line">    from functools import reduce</span><br><span class="line">    </span><br><span class="line">    def add(x, y) :            # 两数相加</span><br><span class="line">        return x + y</span><br><span class="line">    sum1 = reduce(add, [1,2,3,4,5])   # 计算列表和：1+2+3+4+5</span><br><span class="line">    sum2 = reduce(lambda x, y: x+y, [1,2,3,4,5])  # 使用 lambda 匿名函数</span><br><span class="line">    print(sum1)  #15</span><br><span class="line">    print(sum2)  #15</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="0x3-小Tips"><a href="#0x3-小Tips" class="headerlink" title="0x3 小Tips"></a>0x3 小Tips</h2><p>神经元也叫做<strong>感知器</strong></p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409102928489.png" alt="image-20230409102928489"></p>
<p>每个小圈圈都是一个神经元（感知器）</p>
<p>但是每个感知器内部又是这样子：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409103031661.png" alt="image-20230409103031661"></p>
<p>so，代码部分在干什么呢？</p>
<blockquote>
<p>  大步骤：</p>
<ol>
<li>训练感知器</li>
<li>训练好后，我把数据扔给感知器，让他输出结果</li>
</ol>
</blockquote>
<blockquote>
<p>  小步骤-怎么训练感知器？</p>
<ol>
<li><p>创建一个感知器 train_and_perceptron()</p>
</li>
<li><p><mark>十轮迭代训练，速率为0.1<mark>  p.train(input_vecs, labels, 10, 0.1)</p>
<p>每一轮干了这些事情：</p>
<ul>
<li>计算感知器在当前权重下的输出</li>
<li><mark>更新权重<mark></li>
</ul>
</li>
</ol>
</blockquote>
<p>解惑：</p>
<p>在感知器算法中，_update_weights函数是用来更新权重和偏置项的。在每次训练中，该函数会计算当前预测输出与实际标签之间的差异（即误差），并根据误差和学习速率来更新权重和偏置项。</p>
<p>具体来说，对于每个输入向量和对应的标签，算法计算当前输入向量对应的输出值，然后根据预测输出和实际标签之间的差异来计算更新量。对于每个输入向量的每个权重和偏置项，更新量都是根据以下公式计算的：</p>
<p><mark>w’ &#x3D; w + Δw &#x3D; w + η * δ * x b’ &#x3D; b + Δb &#x3D; b + η * δ</mark></p>
<p>其中，w是权重，b是偏置项，w’和b’是更新后的权重和偏置项，η是学习速率，δ是误差，x是当前输入向量的值。<mark>这个公式可以理解为：权重和偏置项的更新量与学习速率和误差成正比，与输入向量的值成正比。</mark></p>
<p>最终，_update_weights函数返回更新后的权重和偏置项。这些更新后的权重和偏置项将用于下一次迭代中，以继续训练模型。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luke-blog.netlify.app/2023/02/26/test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/headIcon.jpg">
      <meta itemprop="name" content="Luke">
      <meta itemprop="description" content="我爱学习！~~~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luke-blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/02/26/test/" class="post-title-link" itemprop="url">test</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-02-26 11:16:48" itemprop="dateCreated datePublished" datetime="2023-02-26T11:16:48+08:00">2023-02-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-11 15:43:58" itemprop="dateModified" datetime="2023-04-11T15:43:58+08:00">2023-04-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>  我爱学习~~</p>
<p>  真的！</p>
</blockquote>
<p>如何快速将今天写的blog进行推送？——hexo三连！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean </span><br><span class="line">hexo g     </span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>



<p>学习思路： n + 100死磕法</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Luke"
      src="/images/headIcon.jpg">
  <p class="site-author-name" itemprop="name">Luke</p>
  <div class="site-description" itemprop="description">我爱学习！~~~</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Luke</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
