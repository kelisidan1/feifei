<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>零基础入门深度学习 - 梯度下降算法 | Luke-blog</title>

  
  <meta name="author" content="John Doe">
  

  
  <meta name="description" content="0x1 这是要干啥？？？🐕🐕🐕目的：我想让误差函数的值变小，咋办？
E是可导函数。
求导！找极小值！然后再看端点！
0x2 画画重点梯度下降算法
  简单说：

计算机不会求极值，but可导函数的极值有个特点：导数为0。
所以让计算机遍历函数，find极小值

  涉及的几个关键词：

梯度下">
  

  
  
  <meta name="keywords" content="">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="零基础入门深度学习 - 梯度下降算法"/>

  <meta property="og:site_name" content="Luke-blog"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Luke-blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Luke-blog</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>零基础入门深度学习 - 梯度下降算法</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2023/04/09/零基础入门深度学习-梯度下降算法/" rel="bookmark">
        <time class="entry-date published" datetime="2023-04-09T07:29:31.000Z">
          2023-04-09
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="0x1-这是要干啥？？？🐕🐕🐕"><a href="#0x1-这是要干啥？？？🐕🐕🐕" class="headerlink" title="0x1 这是要干啥？？？🐕🐕🐕"></a>0x1 这是要干啥？？？🐕🐕🐕</h2><h3 id="目的：我想让误差函数的值变小，咋办？"><a href="#目的：我想让误差函数的值变小，咋办？" class="headerlink" title="目的：我想让误差函数的值变小，咋办？"></a>目的：我想让误差函数的值变小，咋办？</h3><p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409154853341.png" alt="image-20230409154853341"></p>
<p>E是可导函数。</p>
<p>求导！找极小值！然后再看端点！</p>
<h2 id="0x2-画画重点"><a href="#0x2-画画重点" class="headerlink" title="0x2 画画重点"></a>0x2 画画重点</h2><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><blockquote>
<p>  简单说：</p>
</blockquote>
<p>计算机不会求极值，but可导函数的极值有个特点：导数为0。</p>
<p>所以让计算机遍历函数，find极小值</p>
<blockquote>
<p>  涉及的几个关键词：</p>
</blockquote>
<p><mark>梯度下降算法的公式</mark>：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155120140.png" alt="image-20230409155120140">(式1)</p>
<p>那么，我要想求E(w)的极值点，f(x)换成E(w)就行了</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155235721.png" alt="image-20230409155235721">(式2)</p>
<p>∇ E(w)的推导过程目前先不管，它是这样算的：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155359570.png" alt="image-20230409155359570">(式3)</p>
<p>So,把式子3代入式子2就得到了：</p>
<p><img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155519593.png" alt="image-20230409155519593">(式4)</p>
<p>Easy啊~ 一点强度没有！</p>
<h3 id="误差推导"><a href="#误差推导" class="headerlink" title="误差推导"></a>误差推导</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011681952/article/details/93714665">(24条消息) 零基础入门深度学习(2) - 线性单元和梯度下降_Godswisdom的博客-CSDN博客</a></p>
<p><mark>疑惑点：对整个向量求导是什么意思？</mark></p>
<h3 id="随机梯度下降算法-SGD算法"><a href="#随机梯度下降算法-SGD算法" class="headerlink" title="随机梯度下降算法(SGD算法)"></a>随机梯度下降算法(SGD算法)</h3><blockquote>
<p>  why?????</p>
</blockquote>
<p>按照上面提到的<img src="https://raw.githubusercontent.com/kelisidan1/blogImg/main/img/image-20230409155359570.png" alt="image-20230409155359570">来寻找best参数，计算量非常大，更好的solution是SGD算法</p>
<blockquote>
<p>  怎么做的(没有详细介绍)</p>
</blockquote>
<p>在SGD算法中，<mark>每次更新的迭代，只计算一个样本</mark>。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更w 新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新w 并不一定按照减少E 的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少E 的方向前进的，因此最后也能收敛到最小值附近。</p>
<p>对比BGD：</p>
<p><mark>理解：SGD工作的维度是1维，但是次数很多。BGD工作维度为1000000维（夸张的手法），但是次数很少</mark></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>

  









    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2023 John Doe
    
  </p>
</footer>
    
    
  </div>
</div>
</body>
</html>