{"meta":{"title":"Luke-blog","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"零基础入门深度学习 - 神经网络和反向传播算法","slug":"零基础入门深度学习-神经网络和反向传播算法","date":"2023-04-09T12:25:02.000Z","updated":"2023-04-09T13:04:31.240Z","comments":true,"path":"2023/04/09/零基础入门深度学习-神经网络和反向传播算法/","link":"","permalink":"http://example.com/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/","excerpt":"","text":"0x1 它能干啥呢❓手写数字识别？？！！ 0x2 怎么搞？？前面学习的都是训练&#x3D;&#x3D;单个神经元&#x3D;&#x3D;，回顾一下，前面干了些什么： 感知器模拟and运算 预测工资 现在学习的，就是将这些单个神经元（感知器）链接在一起，形成神经网络！ 糟了，感觉太神奇了！ 0x3 记点小笔记🖊 神经元 V.S. 感知器 鲁迅曾说过：神经元与感知器虽说一样，但有点不一样🙈 不一样点：神经元激活函数往往选择为sigmoid函数或tanh函数，而我们说感知器的时候，它的激活函数是阶跃函数 啥是神经网络？ 上面这种是&#x3D;&#x3D;全连接神经网络的结构&#x3D;&#x3D;。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。 神经网络怎么干活？ 如何计算a4的值？ 上面的式子咋来的？ 如何计算y1? 上面的式子咋来的？ 来波小小的summary 简单讲： 输入是 输出是，怎么做到的——神经网络~ 如何用数学来理解？ hidden layer实际上是一个matrix， 向量进去之后，空间发生了扭曲变换，上面的这个变换就像是“二向箔”，形成了新的vector 神经网络的训练 #TODO","categories":[],"tags":[]},{"title":"零基础入门深度学习 - 梯度下降算法","slug":"零基础入门深度学习-梯度下降算法","date":"2023-04-09T07:29:31.000Z","updated":"2023-04-09T11:19:33.650Z","comments":true,"path":"2023/04/09/零基础入门深度学习-梯度下降算法/","link":"","permalink":"http://example.com/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/","excerpt":"","text":"0x1 这是要干啥？？？🐕🐕🐕目的：我想让误差函数的值变小，咋办？ E是可导函数。 求导！找极小值！然后再看端点！ 0x2 画画重点梯度下降算法 简单说： 计算机不会求极值，but可导函数的极值有个特点：导数为0。 所以让计算机遍历函数，find极小值 涉及的几个关键词： &#x3D;&#x3D;梯度下降算法的公式&#x3D;&#x3D;： (式1) 那么，我要想求E(w)的极值点，f(x)换成E(w)就行了 (式2) ∇ E(w)的推导过程目前先不管，它是这样算的： (式3) So,把式子3代入式子2就得到了： (式4) Easy啊~ 一点强度没有！ 误差推导(24条消息) 零基础入门深度学习(2) - 线性单元和梯度下降_Godswisdom的博客-CSDN博客 &#x3D;&#x3D;疑惑点：对整个向量求导是什么意思？&#x3D;&#x3D; 随机梯度下降算法(SGD算法) why????? 按照上面提到的来寻找best参数，计算量非常大，更好的solution是SGD算法 怎么做的(没有详细介绍) 在SGD算法中，&#x3D;&#x3D;每次更新的迭代，只计算一个样本&#x3D;&#x3D;。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更w 新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新w 并不一定按照减少E 的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少E 的方向前进的，因此最后也能收敛到最小值附近。 对比BGD： &#x3D;&#x3D;理解：SGD工作的维度是1维，但是次数很多。BGD工作维度为1000000维（夸张的手法），但是次数很少&#x3D;&#x3D;","categories":[],"tags":[]},{"title":"零基础入门深度学习- 线性单元","slug":"零基础入门深度学习-线性单元和梯度下降","date":"2023-04-09T06:45:12.000Z","updated":"2023-04-09T11:19:01.610Z","comments":true,"path":"2023/04/09/零基础入门深度学习-线性单元和梯度下降/","link":"","permalink":"http://example.com/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/","excerpt":"","text":"0x1这是要干啥？？？通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就&#x3D;&#x3D;掌握了机器学习的基本套路&#x3D;&#x3D; 0x2 自己画画重点线性单元 啥是模型？当我们说模型时，我们实际上在谈论根据输入x 预测输出y 的&#x3D;&#x3D;算法&#x3D;&#x3D; &#x3D;&#x3D;e.g:&#x3D;&#x3D; 如果将 b 写成 w1*x1 ，&#x3D;&#x3D;其中 w1 &#x3D; b 且 x1 &#x3D; 1&#x3D;&#x3D; 那么y &#x3D; w*x + w1* x1 这个式子还可以写成向量的形式： &#x3D;&#x3D;长成这种样子模型就叫做线性模型&#x3D;&#x3D; 监督学习&amp;无监督学习简单讲： 监督学习 -&gt; 有label 类比 一本有答案的习题册，刷完这套题之后去考试（效果嘎嘎好！） 无监督学习 -&gt; 没有 label 类比一本没有答案的习题册，刷完之后去考试（效果不太好） 如何理解输入？类比 我向习题册写的东西，函数的x 如何理解&#x3D;&#x3D;label&#x3D;&#x3D;? 类比 习题册的答案、图像识别的工程帽标签、函数的y 线性单元的目标函数（讲这个的时候，只考虑了监督学习）&#x3D;&#x3D;理解方式：拟合&#x3D;&#x3D; 有了特征x,通过模型h(x)可以计算出来y的估计值。 误差的计算&#x3D;&#x3D;理解： 真值和预测值之间差距有多大啊？ 越大就说明这个模型越不好！&#x3D;&#x3D; 以下推导是非常自然的，一点点看即可~~ 单个样本的误差 N个样本误差的合（注意，以下的几个式子都一样） 上面的&#x3D;&#x3D;y拔&#x3D;&#x3D;就是模型的预测值： 最终版本：🐕🐕🐕 0x3盘一下逻辑⭐⭐⭐⭐⭐开局先自己搞个函数 e.g.: y &#x3D; ax1+bx2+cx3 y &#x3D; WTX 我说这个函数就能解决能&#x3D;&#x3D;预测房价&#x3D;&#x3D;，ok吗？ 答：ok的，我把6月的房价(x1)、销售量(x2)、地段(x3)输入到上面的函数当中，就能完成预测。 but这&#x3D;&#x3D;估计是不准确&#x3D;&#x3D;的，但是能让他准点吗？ 答：能！ &#x3D;&#x3D;有监督学习 + 误差计算&#x3D;&#x3D;不断修改WT的值，就能让他变准点！ 0x4 简单写一下代码，里面有些东西要记住~Python当中的继承 1234f = lambda x: xclass LinearUnit(Perceptron): def __init__(self, input_num): Perceptron.__init__(self, input_num, f)","categories":[],"tags":[]},{"title":"零基础入门深度学习-感知器","slug":"零基础入门深度学习-感知器","date":"2023-04-08T13:17:39.000Z","updated":"2023-04-09T12:13:55.984Z","comments":true,"path":"2023/04/08/零基础入门深度学习-感知器/","link":"","permalink":"http://example.com/2023/04/08/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E5%99%A8/","excerpt":"","text":"0x1 理解链接：https://blog.csdn.net/u011681952/article/details/93633608我的理解： 核心是函数但是要对这个函数进行&#x3D;&#x3D;调参&#x3D;&#x3D;从而能实现线性分类或者线性回归问题e.g. and 函数、or 函数 0x2 从这节课当中学习到的知识 填充list 12weight = [0.0 for _ in range(10)]print(weight) 输出： 1[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] python当中的类（类比Java学习） &#x3D;&#x3D;核心：构造方法、this关键字&#x3D;&#x3D; &#x3D;&#x3D;类比：toString方法&#x3D;&#x3D; 这里有个知识点： 1234\\t 是指制表符，代表着四个空格，也就是一个tab%s %f 是字符串的格式方式self.weights会被填充到 %s的位置上self.bias 会被填充到 %f的位置上 &#x3D;&#x3D;普通方法：&#x3D;&#x3D; 这里有个知识点： activator是在构造函数中有定义，由于python没有数值的类型要求，所以不需要在函数内部再去单独定义这个类的变量 lambda表达式的使用 lambda 参数：返回值 &#x3D;&#x3D;e.g. lambda a,b:a+b&#x3D;&#x3D; 写上它，就相当于说给了一个函数 这个函数可以用在reduce函数当中，进行迭代（目前知道的用途） map的使用 map(function, iterable, …) function —-&gt; 函数iterable —-&gt; 一个或多个序列 &#x3D;&#x3D;经典例子&#x3D;&#x3D; 12345678910111213141516171819# ===========一般写法：===========# 1、计算平方数def square(x): return x ** 2map(square, [1,2,3,4,5]) # 计算列表各个元素的平方# 结果：[1, 4, 9, 16, 25]# ===========匿名函数写法：============# 2、计算平方数，lambda 写法map(lambda x: x ** 2, [1, 2, 3, 4, 5])# 结果：[1, 4, 9, 16, 25] # 3、提供两个列表，将其相同索引位置的列表元素进行相加map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])# 结果：[3, 7, 11, 15, 19] zip函数的使用 1234567x = [1, 2, 3]y = [4, 5, 6]z = [7, 8, 9] xyz = zip(x, y, z)#print xyz运行的结果是：[(1, 4, 7), (2, 5, 8), (3, 6, 9)] 0x3 小Tips神经元也叫做感知器 每个小圈圈都是一个神经元（感知器） 但是每个感知器内部又是这样子： so，代码部分在干什么呢？ 大步骤： 训练感知器 训练好后，我把数据扔给感知器，让他输出结果 小步骤-怎么训练感知器？ 创建一个感知器 train_and_perceptron() &#x3D;&#x3D;十轮迭代训练，速率为0.1&#x3D;&#x3D; p.train(input_vecs, labels, 10, 0.1) 每一轮干了这些事情： 计算感知器在当前权重下的输出 &#x3D;&#x3D;更新权重&#x3D;&#x3D; 解惑： 在感知器算法中，_update_weights函数是用来更新权重和偏置项的。在每次训练中，该函数会计算当前预测输出与实际标签之间的差异（即误差），并根据误差和学习速率来更新权重和偏置项。 具体来说，对于每个输入向量和对应的标签，算法计算当前输入向量对应的输出值，然后根据预测输出和实际标签之间的差异来计算更新量。对于每个输入向量的每个权重和偏置项，更新量都是根据以下公式计算的： &#x3D;&#x3D;w’ &#x3D; w + Δw &#x3D; w + η * δ * x b’ &#x3D; b + Δb &#x3D; b + η * δ&#x3D;&#x3D; 其中，w是权重，b是偏置项，w’和b’是更新后的权重和偏置项，η是学习速率，δ是误差，x是当前输入向量的值。&#x3D;&#x3D;这个公式可以理解为：权重和偏置项的更新量与学习速率和误差成正比，与输入向量的值成正比。&#x3D;&#x3D; 最终，_update_weights函数返回更新后的权重和偏置项。这些更新后的权重和偏置项将用于下一次迭代中，以继续训练模型。","categories":[],"tags":[]},{"title":"test","slug":"test","date":"2023-02-26T03:16:48.000Z","updated":"2023-04-08T13:19:41.030Z","comments":true,"path":"2023/02/26/test/","link":"","permalink":"http://example.com/2023/02/26/test/","excerpt":"","text":"我爱学习~~ 真的！ #TODO 搞定坏掉的PicGo图床！ 如何快速将今天写的blog进行推送？ 123hexo clean hexo g hexo d","categories":[],"tags":[]}],"categories":[],"tags":[]}