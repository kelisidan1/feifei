{"meta":{"title":"Luke-blog","subtitle":"Legends never die！","description":"我爱学习！~~~","author":"Luke","url":"https://luke-blog.netlify.app","root":"/"},"pages":[{"title":"tages","date":"2023-04-10T11:08:25.000Z","updated":"2023-04-10T11:08:25.947Z","comments":true,"path":"tages/index.html","permalink":"https://luke-blog.netlify.app/tages/index.html","excerpt":"","text":""},{"title":"categories","date":"2023-04-10T11:09:42.000Z","updated":"2023-04-10T11:09:42.515Z","comments":true,"path":"categories/index.html","permalink":"https://luke-blog.netlify.app/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"零基础入门深度学习-循环神经网络","slug":"零基础入门深度学习-循环神经网络","date":"2023-04-13T08:59:46.000Z","updated":"2023-04-15T13:06:19.667Z","comments":true,"path":"2023/04/13/零基础入门深度学习-循环神经网络/","link":"","permalink":"https://luke-blog.netlify.app/2023/04/13/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"0x 0 一种形象化的理解方式 一定要看这个：【循环神经网络】5分钟搞懂RNN，3D动画深入浅出_哔哩哔哩_bilibili 前置知识：矩阵对矩阵的求导、矩阵对向量的求导运算 0x 1 RNN能干啥？ RNN可以为语言模型来建模 比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：循环神经网络(Recurrent Neural Network)。 0x 2 RNN理论知识学习最简单的RNN张什么样 它会有这样一个式子： 反复将式2代入式1，就有： So，输入的前半部分会影响到后半部分的输出。 上点强度——双向RNN有两个W，但是这里用A表示。 A1用于正向计算 A2用于反向计算 循环神经网络的训练 BPTT算法 针对循环层的训练算法，原理与BP一致 步骤： 前向计算求结果 反向计算求误差 随机下降更权重 误差怎么求 怎么计算权重的梯度？博客当中对矩阵求导，但是我们之前几个博客都是对向量进行求导。 对向量求导比一个一个对变量求导快（支持GPU加速且形式更美观） 本质相同，但是形式不同 还是两方面考虑 Tk时刻的前层权重更新就和之前的全连接网络更新梯度一样 Tk时刻计算Tk-s的梯度是我们要新考虑的东西 权重矩阵U和W的计算方法： ∇ 是梯度算子，∇ f ( x )就是指f ( x ) 的梯度。 最后为什么是加法呢？见下面的推导 RNN的梯度爆炸和消失问题梯度消失 &#x3D;&gt; 训练缓慢 甚至停滞 梯度爆炸 &#x3D;&gt; 可能不收敛 RNN的应用——语言模型我当前输入一个词，循环神经网络会帮我预测下一个词要输出什么 Solution: 概率。 But,这个概率是怎么来的？ Softmax函数 误差计算","categories":[{"name":"零基础入门深度学习","slug":"零基础入门深度学习","permalink":"https://luke-blog.netlify.app/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"零基础入门深度学习 - 卷积神经网络","slug":"零基础入门深度学习-卷积神经网络","date":"2023-04-11T02:47:20.000Z","updated":"2023-04-13T08:58:56.549Z","comments":true,"path":"2023/04/11/零基础入门深度学习-卷积神经网络/","link":"","permalink":"https://luke-blog.netlify.app/2023/04/11/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"0x 1 它可以做什么？图像分类 0x 2 博客学习Relu函数 （新的激活函数） 它的图像张这样： 为啥要用Relu? 速度快 减轻梯度消失问题(啥意思？) 稀疏性 啥是卷积神经网络 感性认识 一张彩色图片 &#x3D; 3-D tensor 哪三个d呢？ 图片宽 W 图片高 H 图片的 channels 图像分类其实不需要使用全连接，原因是：我只需要图像当中个别的几个特征（非常有辨识度的），就能够辨别出这是个什么玩意，就是下图所示的这种感觉~~ 使用全连接就有些像扔给神经元的全是完整的图片。 怎么做呢？ 同样的特征可能出现图片当中的不同地方，像上面的这种全地图扫描的方式，确实不会漏掉特征，但是还是整个系统还是有些庞大~ Solution: 共享参数（weight一样），但是field不一样 Q：从input image 到 feature maps 1-&gt;3的这个数量变化是怎么来的？ A: 每一个filter进行一次卷积后都会得到一个Feature map,所以从input image 到 feature maps应该是有三个filter。 简单说： Convolution Layer使用Filter得到Feature Maps (寻找特征的过程) Pooling Layer是对Feature Map进行采样的过程 (寻找最明显特征的过程) Filter是怎么工作的？stride &#x3D; 1（步幅为1） stride &#x3D; 2时： 所以说，最终的Feature Map大小有个公式 W2是卷积后Feature Map的宽度 W1是卷积前图像的宽度 F是filter的宽度 P是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果的值是1，那么就补1圈0； S是步幅； H2是卷积后Feature Map的高度； H1是卷积前图像的宽度 深度为1的卷积层计算方法： 深度大于1的卷积层计算方法： 例如： 上面的图意思就是：有三个Channel，两个filter的计算方法，立体一点的话，就是这样子： but，上面的公式是不是有点复杂，能不能精简一下？ 能，但是只能在stride&#x3D;1的情况下进行精简——卷积公式 卷积公式 我们在概率论当中学习过卷积公式，但是数学的卷积公式和卷积神经网络当中的卷积是有区别的： Pooling 层输出值的计算 取出Feature Map当中最重要的样本，从而减少参数数量。 这里用的是Max Pooling。 其他的还有Mean Pooling…. 全连接层没啥说的 卷积神经网络的训练和全连接神经网络相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的： 利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权重。训练算法依然是反向传播算法。 卷积层的误差项传递 (注意是误差项(Ed对net求导)，不是误差(Ed)) 步长为1时的误差 步长&gt;1时的误差(从步长为1的里面挑一挑就是了)： 卷积层 filter权重梯度的计算（有疑问） 理解：image经过多个filter会产生多个channel，如果做反向传播的话，生成的三个channel对原来的image的误差是有”叠加“效果的。 Pooling层的训练 无论max pooling还是mean pooling，都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算。 0x 3 python小知识random函数的用法code： 12345import numpy as npweights = np.random.uniform(-1e-4,1e-4,(2,3,3))bias = 0weight_grad = np.zeros(weights.shape)print(weight_grad) output: 1234567[[[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]]] 魔术方法 _repr_ 就是说，实例化一个类的时候，会有一个回显 np.nditer()函数123456789import numpy as npa = np.arange(6).reshape(2,3)# np.arange(6) 得到：[0 1 2 3 4 5]# 再reshape一下，得到：[[0 1 2] [3 4 5]]with np.nditer(a,op_flags = [&#x27;readwrite&#x27;]) as it: #np.nditer(arrar,op_flags)会生成一个迭代器，我们叫他it for x in it: #使用nditer迭代器进行循环，速度要比for-loop快很多倍 x[...] = 2*x #x[...]是一种“扩展切片”（Ellipsis slicing）的写法，它表示对数组中所有元素进行操作。print(a) 在做padding的时候的代码解析：1234567891011121314151617181920212223242526if input_array.ndim == 3: #ndim 是数组的维度 input_width = input_array.shape[2] input_height = input_array.shape[1] input_depth = input_array.shape[0] padded_array = np.zeros(( input_depth, input_height + 2*zp, input_width + 2*zp )) padded_array[:, zp:zp + input_height, zp:zp +input_width ] = input_array return padded_arrayelif input_array.ndim==2: input_width = input_array.shape[1] input_height = input_array.shpe[0] padded_array = np.zeros(( input_height + 2*zp, input_width + 2*zp )) padded_array[ zp:zp+input_height+2*zp, zp:zp + input_width ] = input_array return padded_array 核心考察点：numpy数组当中，冒号的使用 简单说流程： 找到填充的位置，进行填充 使用numpy数组的时候，它完全就是一个向量&#x2F;矩阵 可以去做向量减法、矩阵乘法等等操作。。。 numpy数组的shape属性二维： shape[0] ：有几行 shape[1]：有几列 三维： shape[0] ：有几层 （channel数） shape[1] ： 有几行 shape[2] : 有几列 0x 4 代码实现盘逻辑 &amp; debug以下为矢量图，可以下载下来放大看","categories":[{"name":"零基础入门深度学习","slug":"零基础入门深度学习","permalink":"https://luke-blog.netlify.app/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"零基础入门深度学习 - 神经网络和反向传播算法","slug":"零基础入门深度学习-神经网络和反向传播算法","date":"2023-04-09T12:25:02.000Z","updated":"2023-04-15T00:45:34.296Z","comments":true,"path":"2023/04/09/零基础入门深度学习-神经网络和反向传播算法/","link":"","permalink":"https://luke-blog.netlify.app/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/","excerpt":"","text":"最简单的理解方法 0x0 前置知识学的有啥用？ 0x1 这节课学的东西，它能干啥呢❓手写数字识别？？！！ 0x2 怎么搞？？前面学习的都是训练单个神经元，回顾一下，前面干了些什么： 感知器模拟and运算 预测工资 现在学习的，就是将这些单个神经元（感知器）链接在一起，形成神经网络！ 糟了，感觉太神奇了！ 0x3 理论学习🖊神经元 V.S. 感知器鲁迅曾说过：神经元与感知器虽说一样，但有点不一样🙈 不一样点：神经元激活函数往往选择为sigmoid函数或tanh函数，而我们说感知器的时候，它的激活函数是阶跃函数 啥是神经网络？ 上面这种是全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。 神经网络怎么干活？ 如何计算a4的值？ 上面的式子咋来的？ 如何计算y1? 上面的式子咋来的？ 来波小小的summary 简单讲： 输入是 输出是，怎么做到的——神经网络~ 如何用数学来理解？ hidden layer实际上是一个matrix， 向量进去之后，空间发生了扭曲变换，上面的这个变换就像是“二向箔”，形成了新的vector 神经网络的训练神经网络是一个模型 权值是模型的参数 一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为**超参数(Hyper-Parameters)**。 反向传播算法(通过结果看原因)⭐⭐⭐⭐一句话说：从后向前，利用误差项的计算和权重更新方法，计算出wji更新所需要用到的δ 啥意思？为啥叫反向传播算法？ 举个有监督模型的例子：高中语文阅读理解题（特点：主观性强） 简单说： 有监督&#x3D;&gt;有答案的练习册，我做完题之后可以对答案 对答案时，我发现我写的不太好 怎么不好呢？&#x3D;&gt; 输出的Y 和我 答案上的 T相差比较大 (通过误差项估计可以看出来) 那写的不好（output layer），我就要找找原因了，也许是学习方法的问题 我先根据误差，对自己写的答案进行了一点修改 （到了hidden layer） 修改了一点后，我悟了，原来是最开始的时候，审题审的不好，那么再根据误差修改我的审题思路(到了 input layer) Q&amp;A: Q：我改的是什么？ A：改的其实是每一个神经元的w,也就是权重 Q：改完之后，能干啥啊？ A：改完之后，输出就更接近真实的答案了。给分也就会更高。 用数学理解 反向传播算法（🐕🐕🐕） 用矩阵来描述上面的这个神经网络： 令： 从而： 最终有： 更新权值相关公式（这在5.2.1、5.2.2中有推导过程）： 能用这个式子的理由：梯度下降法 这段话非常重要：显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后，我们就可以根据式5来更新所有的权重。 看看，式子5当中的 η是学习速率 δ j是节点j 的误差项（注意:误差项和误差不是一个东西） xji是节点i传递给节点j的输入 我的目的：让输出T与Y的误差变小 误差怎么估计？ &#x3D;&gt; 梯度下降法：出误差E d对于每个权重wji的偏导数（也就是梯度）这就有回到了式(5) 计算出误差E d对于每个权重wji的偏导数的目的：修改wji 修改wji的目的：让误差变小 0x4 python语法知识补充 🎈a[-1] V.S. a[:-1] V.S. a[::-1] 0x5 写写代码、搞懂整体逻辑 0x6 如何保证自己的神经网络没有BUG?核心： 将神经网络代码算出来的梯度值和手算出来的梯度值进行比较，如果差别比较小，就说明代码是正确的。 0x7 手写数字识别准备数据集：MNIST数据集 超参数超参数（hyperparameters）是指机器学习模型中需要手动设置的参数，这些参数不能通过训练数据自动学习得到。 通常，超参数需要在训练过程前手动调整，以优化模型的性能和准确性。 超参数通常包括模型的学习率、正则化参数、批量大小、层数、神经元数量等。这些参数的设置可以对模型的效果产生重要影响，因此超参数的选择是非常重要的。 为了找到最佳的超参数组合，可以使用交叉验证等技术来评估模型的性能，并尝试不同的超参数值来找到最佳的超参数组合。 模型评估错误率&#x3D;错误预测样本数&#x2F;总样本数 0x8 向量化编程区别于面向对象、面向过程 why? &#x3D;&gt; GPU可以对向量运算进行优化 通过上面的这一个公式，来实现全连接层向前和向后计算 对比面向对象的写法： 面向对象是一步一步向前面的Layer迭代的，向量化是对整个网络直接操作的 0x 9手推公式","categories":[{"name":"零基础入门深度学习","slug":"零基础入门深度学习","permalink":"https://luke-blog.netlify.app/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"零基础入门深度学习 - 梯度下降算法","slug":"零基础入门深度学习-梯度下降算法","date":"2023-04-09T07:29:31.000Z","updated":"2023-04-11T08:17:26.136Z","comments":true,"path":"2023/04/09/零基础入门深度学习-梯度下降算法/","link":"","permalink":"https://luke-blog.netlify.app/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/","excerpt":"","text":"0x1 这是要干啥？？？🐕🐕🐕目的：我想让误差函数的值变小，咋办？ E是可导函数。 求导！找极小值！然后再看端点！ 0x2 画画重点梯度下降算法 简单说： 计算机不会求极值，but可导函数的极值有个特点：导数为0。 所以让计算机遍历函数，find极小值 涉及的几个关键词： 梯度下降算法的公式： (式1) 那么，我要想求E(w)的极值点，f(x)换成E(w)就行了 (式2) ∇ E(w)的推导过程目前先不管，它是这样算的： (式3) So,把式子3代入式子2就得到了： (式4) Easy啊~ 一点强度没有！ 误差推导(24条消息) 零基础入门深度学习(2) - 线性单元和梯度下降_Godswisdom的博客-CSDN博客 疑惑点：对整个向量求导是什么意思？ 随机梯度下降算法(SGD算法) why????? 按照上面提到的来寻找best参数，计算量非常大，更好的solution是SGD算法 怎么做的(没有详细介绍) 在SGD算法中，每次更新的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更w 新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新w 并不一定按照减少E 的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少E 的方向前进的，因此最后也能收敛到最小值附近。 对比BGD： 理解：SGD工作的维度是1维，但是次数很多。BGD工作维度为1000000维（夸张的手法），但是次数很少","categories":[{"name":"零基础入门深度学习","slug":"零基础入门深度学习","permalink":"https://luke-blog.netlify.app/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"零基础入门深度学习- 线性单元","slug":"零基础入门深度学习-线性单元","date":"2023-04-09T06:45:12.000Z","updated":"2023-04-10T23:39:45.409Z","comments":true,"path":"2023/04/09/零基础入门深度学习-线性单元/","link":"","permalink":"https://luke-blog.netlify.app/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83/","excerpt":"","text":"0x1这是要干啥？？？通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就掌握了机器学习的基本套路 0x2 自己画画重点线性单元 啥是模型？当我们说模型时，我们实际上在谈论根据输入x 预测输出y 的算法 e.g: 如果将 b 写成 w1*x1 ，其中 w1 &#x3D; b 且 x1 &#x3D; 1 那么y &#x3D; w*x + w1* x1 这个式子还可以写成向量的形式： 长成这种样子模型就叫做线性模型 监督学习&amp;无监督学习简单讲： 监督学习 -&gt; 有label 类比 一本有答案的习题册，刷完这套题之后去考试（效果嘎嘎好！） 无监督学习 -&gt; 没有 label 类比一本没有答案的习题册，刷完之后去考试（效果不太好） 如何理解输入？类比 我向习题册写的东西，函数的x 如何理解label? 类比 习题册的答案、图像识别的工程帽标签、函数的y 线性单元的目标函数（讲这个的时候，只考虑了监督学习）理解方式：拟合 有了特征x,通过模型h(x)可以计算出来y的估计值。 误差的计算理解： 真值和预测值之间差距有多大啊？ 越大就说明这个模型越不好！ 以下推导是非常自然的，一点点看即可~~ 单个样本的误差 N个样本误差的合（注意，以下的几个式子都一样） 上面的y拔就是模型的预测值： 最终版本：🐕🐕🐕 0x3盘一下逻辑⭐⭐⭐⭐⭐开局先自己搞个函数 e.g.: y &#x3D; ax1+bx2+cx3 y &#x3D; WTX 我说这个函数就能解决能预测房价，ok吗？ 答：ok的，我把6月的房价(x1)、销售量(x2)、地段(x3)输入到上面的函数当中，就能完成预测。 but这估计是不准确的，但是能让他准点吗？ 答：能！ 有监督学习 + 误差计算不断修改WT的值，就能让他变准点！ 0x4 简单写一下代码，里面有些东西要记住~Python当中的继承 1234f = lambda x: xclass LinearUnit(Perceptron): def __init__(self, input_num): Perceptron.__init__(self, input_num, f)","categories":[{"name":"零基础入门深度学习","slug":"零基础入门深度学习","permalink":"https://luke-blog.netlify.app/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"零基础入门深度学习-感知器","slug":"零基础入门深度学习-感知器","date":"2023-04-08T13:17:39.000Z","updated":"2023-04-10T23:39:34.995Z","comments":true,"path":"2023/04/08/零基础入门深度学习-感知器/","link":"","permalink":"https://luke-blog.netlify.app/2023/04/08/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E5%99%A8/","excerpt":"","text":"0x1 理解链接：https://blog.csdn.net/u011681952/article/details/93633608我的理解： 核心是函数但是要对这个函数进行调参从而能实现线性分类或者线性回归问题e.g. and 函数、or 函数 0x2 从这节课当中学习到的知识 填充list 12weight = [0.0 for _ in range(10)]print(weight) 输出： 1[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] python当中的类（类比Java学习） 核心：构造方法、this关键字 类比：toString方法 这里有个知识点： 1234\\t 是指制表符，代表着四个空格，也就是一个tab%s %f 是字符串的格式方式self.weights会被填充到 %s的位置上self.bias 会被填充到 %f的位置上 普通方法： 这里有个知识点： activator是在构造函数中有定义，由于python没有数值的类型要求，所以不需要在函数内部再去单独定义这个类的变量 lambda表达式的使用 lambda 参数：返回值 e.g. lambda a,b:a+b 写上它，就相当于说给了一个函数 这个函数可以用在reduce函数当中，进行迭代（目前知道的用途） map的使用 map(function, iterable, …) 它返回的是一个Map对象，如果想要输出里面的内容，可以转成List集合进行输出 function —-&gt; 函数iterable —-&gt; 一个或多个序列 经典例子 12345678910111213141516171819# &lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=一般写法：&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=# 1、计算平方数def square(x): return x ** 2map(square, [1,2,3,4,5]) # 计算列表各个元素的平方# 结果：[1, 4, 9, 16, 25]# &lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=匿名函数写法：&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;# 2、计算平方数，lambda 写法map(lambda x: x ** 2, [1, 2, 3, 4, 5])# 结果：[1, 4, 9, 16, 25] # 3、提供两个列表，将其相同索引位置的列表元素进行相加map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])# 结果：[3, 7, 11, 15, 19] zip函数的使用 1234567x = [1, 2, 3]y = [4, 5, 6]z = [7, 8, 9] xyz = zip(x, y, z)#print xyz运行的结果是：[(1, 4, 7), (2, 5, 8), (3, 6, 9)] reduce函数的使用 描述：reduce() 函数会对参数序列中元素进行累积。 函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。 &#96;&#96;&#96;python 用法：reduce(function, iterable[, initializer]) function – 函数，有两个参数iterable – 可迭代对象initializer – 可选，初始参数12345678910111213+ 例子： ```python #!/usr/bin/python from functools import reduce def add(x, y) : # 两数相加 return x + y sum1 = reduce(add, [1,2,3,4,5]) # 计算列表和：1+2+3+4+5 sum2 = reduce(lambda x, y: x+y, [1,2,3,4,5]) # 使用 lambda 匿名函数 print(sum1) #15 print(sum2) #15 0x3 小Tips神经元也叫做感知器 每个小圈圈都是一个神经元（感知器） 但是每个感知器内部又是这样子： so，代码部分在干什么呢？ 大步骤： 训练感知器 训练好后，我把数据扔给感知器，让他输出结果 小步骤-怎么训练感知器？ 创建一个感知器 train_and_perceptron() 十轮迭代训练，速率为0.1 p.train(input_vecs, labels, 10, 0.1) 每一轮干了这些事情： 计算感知器在当前权重下的输出 更新权重 解惑： 在感知器算法中，_update_weights函数是用来更新权重和偏置项的。在每次训练中，该函数会计算当前预测输出与实际标签之间的差异（即误差），并根据误差和学习速率来更新权重和偏置项。 具体来说，对于每个输入向量和对应的标签，算法计算当前输入向量对应的输出值，然后根据预测输出和实际标签之间的差异来计算更新量。对于每个输入向量的每个权重和偏置项，更新量都是根据以下公式计算的： w’ &#x3D; w + Δw &#x3D; w + η * δ * x b’ &#x3D; b + Δb &#x3D; b + η * δ 其中，w是权重，b是偏置项，w’和b’是更新后的权重和偏置项，η是学习速率，δ是误差，x是当前输入向量的值。这个公式可以理解为：权重和偏置项的更新量与学习速率和误差成正比，与输入向量的值成正比。 最终，_update_weights函数返回更新后的权重和偏置项。这些更新后的权重和偏置项将用于下一次迭代中，以继续训练模型。","categories":[{"name":"零基础入门深度学习","slug":"零基础入门深度学习","permalink":"https://luke-blog.netlify.app/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"test","slug":"test","date":"2023-02-26T03:16:48.000Z","updated":"2023-04-11T07:43:58.824Z","comments":true,"path":"2023/02/26/test/","link":"","permalink":"https://luke-blog.netlify.app/2023/02/26/test/","excerpt":"","text":"我爱学习~~ 真的！ 如何快速将今天写的blog进行推送？——hexo三连！ 123hexo clean hexo g hexo d 学习思路： n + 100死磕法","categories":[],"tags":[]}],"categories":[{"name":"零基础入门深度学习","slug":"零基础入门深度学习","permalink":"https://luke-blog.netlify.app/categories/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]}