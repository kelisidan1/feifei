{"meta":{"title":"Luke-blog","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"零基础入门深度学习 - 神经网络和反向传播算法","slug":"零基础入门深度学习-神经网络和反向传播算法","date":"2023-04-09T12:25:02.000Z","updated":"2023-04-10T05:41:36.801Z","comments":true,"path":"2023/04/09/零基础入门深度学习-神经网络和反向传播算法/","link":"","permalink":"http://example.com/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/","excerpt":"","text":"0x1 它能干啥呢❓手写数字识别？？！！ 0x2 怎么搞？？前面学习的都是训练单个神经元，回顾一下，前面干了些什么： 感知器模拟and运算 预测工资 现在学习的，就是将这些单个神经元（感知器）链接在一起，形成神经网络！ 糟了，感觉太神奇了！ 0x3 记点小笔记🖊神经元 V.S. 感知器鲁迅曾说过：神经元与感知器虽说一样，但有点不一样🙈 不一样点：神经元激活函数往往选择为sigmoid函数或tanh函数，而我们说感知器的时候，它的激活函数是阶跃函数 啥是神经网络？ 上面这种是全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。 神经网络怎么干活？ 如何计算a4的值？ 上面的式子咋来的？ 如何计算y1? 上面的式子咋来的？ 来波小小的summary 简单讲： 输入是 输出是，怎么做到的——神经网络~ 如何用数学来理解？ hidden layer实际上是一个matrix， 向量进去之后，空间发生了扭曲变换，上面的这个变换就像是“二向箔”，形成了新的vector 神经网络的训练神经网络是一个模型 权值是模型的参数 一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为**超参数(Hyper-Parameters)**。 反向传播算法(通过结果看原因) 啥意思？为啥叫反向传播算法？ 举个有监督模型的例子：高中语文阅读理解题（特点：主观性强） 简单说： 有监督&#x3D;&gt;有答案的练习册，我做完题之后可以对答案 对答案时，我发现我写的不太好 怎么不好呢？&#x3D;&gt; 输出的Y 和我 答案上的 T相差比较大 (通过误差项估计可以看出来) 那写的不好（output layer），我就要找找原因了，也许是学习方法的问题 我先根据误差，对自己写的答案进行了一点修改 （到了hidden layer） 修改了一点后，我悟了，原来是最开始的时候，审题审的不好，那么再根据误差修改我的审题思路(到了 input layer) Q&amp;A: Q：我改的是什么？ A：改的其实是每一个神经元的w,也就是权重 Q：改完之后，能干啥啊？ A：改完之后，输出就更接近真实的答案了。给分也就会更高。 用数学理解 反向传播算法（🐕🐕🐕） 我的目的：让输出T与Y的误差变小 误差怎么估计？ &#x3D;&gt; 怎么让误差变小？ 梯度下降法：出误差E d对于每个权重wji的偏导数（也就是梯度） 0x4 python语法知识补充 🎈a[-1] V.S. a[:-1] V.S. a[::-1] 0x5 需要对这几天学习的知识进行一下总结#TODO","categories":[],"tags":[]},{"title":"零基础入门深度学习 - 梯度下降算法","slug":"零基础入门深度学习-梯度下降算法","date":"2023-04-09T07:29:31.000Z","updated":"2023-04-10T05:42:06.749Z","comments":true,"path":"2023/04/09/零基础入门深度学习-梯度下降算法/","link":"","permalink":"http://example.com/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/","excerpt":"","text":"0x1 这是要干啥？？？🐕🐕🐕目的：我想让误差函数的值变小，咋办？ E是可导函数。 求导！找极小值！然后再看端点！ 0x2 画画重点梯度下降算法 简单说： 计算机不会求极值，but可导函数的极值有个特点：导数为0。 所以让计算机遍历函数，find极小值 涉及的几个关键词： 梯度下降算法的公式： (式1) 那么，我要想求E(w)的极值点，f(x)换成E(w)就行了 (式2) ∇ E(w)的推导过程目前先不管，它是这样算的： (式3) So,把式子3代入式子2就得到了： (式4) Easy啊~ 一点强度没有！ 误差推导(24条消息) 零基础入门深度学习(2) - 线性单元和梯度下降_Godswisdom的博客-CSDN博客 疑惑点：对整个向量求导是什么意思？ 随机梯度下降算法(SGD算法) why????? 按照上面提到的来寻找best参数，计算量非常大，更好的solution是SGD算法 怎么做的(没有详细介绍) 在SGD算法中，每次更新的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更w 新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新w 并不一定按照减少E 的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少E 的方向前进的，因此最后也能收敛到最小值附近。 对比BGD： 理解：SGD工作的维度是1维，但是次数很多。BGD工作维度为1000000维（夸张的手法），但是次数很少","categories":[],"tags":[]},{"title":"零基础入门深度学习- 线性单元","slug":"零基础入门深度学习-线性单元和梯度下降","date":"2023-04-09T06:45:12.000Z","updated":"2023-04-10T05:42:52.186Z","comments":true,"path":"2023/04/09/零基础入门深度学习-线性单元和梯度下降/","link":"","permalink":"http://example.com/2023/04/09/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/","excerpt":"","text":"0x1这是要干啥？？？通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就掌握了机器学习的基本套路 0x2 自己画画重点线性单元 啥是模型？当我们说模型时，我们实际上在谈论根据输入x 预测输出y 的算法 e.g: 如果将 b 写成 w1*x1 ，其中 w1 &#x3D; b 且 x1 &#x3D; 1 那么y &#x3D; w*x + w1* x1 这个式子还可以写成向量的形式： 长成这种样子模型就叫做线性模型 监督学习&amp;无监督学习简单讲： 监督学习 -&gt; 有label 类比 一本有答案的习题册，刷完这套题之后去考试（效果嘎嘎好！） 无监督学习 -&gt; 没有 label 类比一本没有答案的习题册，刷完之后去考试（效果不太好） 如何理解输入？类比 我向习题册写的东西，函数的x 如何理解label? 类比 习题册的答案、图像识别的工程帽标签、函数的y 线性单元的目标函数（讲这个的时候，只考虑了监督学习）理解方式：拟合 有了特征x,通过模型h(x)可以计算出来y的估计值。 误差的计算理解： 真值和预测值之间差距有多大啊？ 越大就说明这个模型越不好！ 以下推导是非常自然的，一点点看即可~~ 单个样本的误差 N个样本误差的合（注意，以下的几个式子都一样） 上面的y拔就是模型的预测值： 最终版本：🐕🐕🐕 0x3盘一下逻辑⭐⭐⭐⭐⭐开局先自己搞个函数 e.g.: y &#x3D; ax1+bx2+cx3 y &#x3D; WTX 我说这个函数就能解决能预测房价，ok吗？ 答：ok的，我把6月的房价(x1)、销售量(x2)、地段(x3)输入到上面的函数当中，就能完成预测。 but这估计是不准确的，但是能让他准点吗？ 答：能！ 有监督学习 + 误差计算不断修改WT的值，就能让他变准点！ 0x4 简单写一下代码，里面有些东西要记住~Python当中的继承 1234f = lambda x: xclass LinearUnit(Perceptron): def __init__(self, input_num): Perceptron.__init__(self, input_num, f)","categories":[],"tags":[]},{"title":"零基础入门深度学习-感知器","slug":"零基础入门深度学习-感知器","date":"2023-04-08T13:17:39.000Z","updated":"2023-04-10T05:41:04.547Z","comments":true,"path":"2023/04/08/零基础入门深度学习-感知器/","link":"","permalink":"http://example.com/2023/04/08/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E5%99%A8/","excerpt":"","text":"0x1 理解链接：https://blog.csdn.net/u011681952/article/details/93633608我的理解： 核心是函数但是要对这个函数进行调参从而能实现线性分类或者线性回归问题e.g. and 函数、or 函数 0x2 从这节课当中学习到的知识 填充list 12weight = [0.0 for _ in range(10)]print(weight) 输出： 1[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] python当中的类（类比Java学习） 核心：构造方法、this关键字 类比：toString方法 这里有个知识点： 1234\\t 是指制表符，代表着四个空格，也就是一个tab%s %f 是字符串的格式方式self.weights会被填充到 %s的位置上self.bias 会被填充到 %f的位置上 普通方法： 这里有个知识点： activator是在构造函数中有定义，由于python没有数值的类型要求，所以不需要在函数内部再去单独定义这个类的变量 lambda表达式的使用 lambda 参数：返回值 e.g. lambda a,b:a+b 写上它，就相当于说给了一个函数 这个函数可以用在reduce函数当中，进行迭代（目前知道的用途） map的使用 map(function, iterable, …) 它返回的是一个Map对象，如果想要输出里面的内容，可以转成List集合进行输出 function —-&gt; 函数iterable —-&gt; 一个或多个序列 经典例子 12345678910111213141516171819# &lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=一般写法：&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=# 1、计算平方数def square(x): return x ** 2map(square, [1,2,3,4,5]) # 计算列表各个元素的平方# 结果：[1, 4, 9, 16, 25]# &lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;=匿名函数写法：&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;&lt;mark&gt;# 2、计算平方数，lambda 写法map(lambda x: x ** 2, [1, 2, 3, 4, 5])# 结果：[1, 4, 9, 16, 25] # 3、提供两个列表，将其相同索引位置的列表元素进行相加map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])# 结果：[3, 7, 11, 15, 19] zip函数的使用 1234567x = [1, 2, 3]y = [4, 5, 6]z = [7, 8, 9] xyz = zip(x, y, z)#print xyz运行的结果是：[(1, 4, 7), (2, 5, 8), (3, 6, 9)] reduce函数的使用 描述：reduce() 函数会对参数序列中元素进行累积。 函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。 &#96;&#96;&#96;python 用法：reduce(function, iterable[, initializer]) function – 函数，有两个参数iterable – 可迭代对象initializer – 可选，初始参数12345678910111213+ 例子： ```python #!/usr/bin/python from functools import reduce def add(x, y) : # 两数相加 return x + y sum1 = reduce(add, [1,2,3,4,5]) # 计算列表和：1+2+3+4+5 sum2 = reduce(lambda x, y: x+y, [1,2,3,4,5]) # 使用 lambda 匿名函数 print(sum1) #15 print(sum2) #15 0x3 小Tips神经元也叫做感知器 每个小圈圈都是一个神经元（感知器） 但是每个感知器内部又是这样子： so，代码部分在干什么呢？ 大步骤： 训练感知器 训练好后，我把数据扔给感知器，让他输出结果 小步骤-怎么训练感知器？ 创建一个感知器 train_and_perceptron() 十轮迭代训练，速率为0.1 p.train(input_vecs, labels, 10, 0.1) 每一轮干了这些事情： 计算感知器在当前权重下的输出 更新权重 解惑： 在感知器算法中，_update_weights函数是用来更新权重和偏置项的。在每次训练中，该函数会计算当前预测输出与实际标签之间的差异（即误差），并根据误差和学习速率来更新权重和偏置项。 具体来说，对于每个输入向量和对应的标签，算法计算当前输入向量对应的输出值，然后根据预测输出和实际标签之间的差异来计算更新量。对于每个输入向量的每个权重和偏置项，更新量都是根据以下公式计算的： w’ &#x3D; w + Δw &#x3D; w + η * δ * x b’ &#x3D; b + Δb &#x3D; b + η * δ 其中，w是权重，b是偏置项，w’和b’是更新后的权重和偏置项，η是学习速率，δ是误差，x是当前输入向量的值。这个公式可以理解为：权重和偏置项的更新量与学习速率和误差成正比，与输入向量的值成正比。 最终，_update_weights函数返回更新后的权重和偏置项。这些更新后的权重和偏置项将用于下一次迭代中，以继续训练模型。","categories":[],"tags":[]},{"title":"test","slug":"test","date":"2023-02-26T03:16:48.000Z","updated":"2023-04-10T05:43:12.601Z","comments":true,"path":"2023/02/26/test/","link":"","permalink":"http://example.com/2023/02/26/test/","excerpt":"","text":"我爱学习~~ 真的！ 如何快速将今天写的blog进行推送？——hexo三连！ 123hexo clean hexo g hexo d","categories":[],"tags":[]}],"categories":[],"tags":[]}